# Ścieżka do binarki Ollama
OLLAMA_PATH=/usr/local/bin/ollama

# Model domyślny do używania (dostępne modele można sprawdzić przez 'ollama list')
OLLAMA_MODEL=phi:2.7b

# Alternatywne modele do wykorzystania w przypadku braku domyślnego
OLLAMA_FALLBACK_MODELS=llama3.2,phi3,qwen,phi,tinyllama

# Ustawienia Pythona
PYTHON_PATH=python3

# Debug mode
DEBUG=true
PYTHON_VENV=.venv

# Ustawienia Docker
DOCKER_IMAGE=ollama/ollama:latest
DOCKER_CONTAINER_NAME=pylama-sandbox
DOCKER_PORT=11434

# Katalogi
LOG_DIR=./logs
OUTPUT_DIR=./output
SCRIPTS_DIR=./scripts
MODELS_DIR=./models

# Przykładowe prompty testowe
TEST_PROMPT_1="create the sentence as python code: Create screenshot on browser"
TEST_PROMPT_2="write a python script that downloads an image from a URL and saves it to disk"
TEST_PROMPT_3="create python code that reads a CSV file and displays its contents in a table"

# Debug
DEBUG_MODE=True
SAVE_RAW_RESPONSES=True

# WebLama API Configuration
API_URL=http://localhost:8081
API_PORT=8081
API_HOST=localhost
MARKDOWN_DIR=~/github/py-lama/weblama/markdown